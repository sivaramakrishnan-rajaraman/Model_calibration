{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04bfc499",
   "metadata": {},
   "source": [
    "### The following code could be used to calibrate model outputs and to investigate if model calibration improved performance compared to the baseline unclibrated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a076410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare gpu use\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear warnings and session\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore',category=FutureWarning) \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e56929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import other libraries\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "from matplotlib import pyplot\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import math\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.applications import VGG16, DenseNet121, InceptionV3\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\n",
    "from sklearn.metrics import roc_curve, auc,  precision_recall_curve, average_precision_score, matthews_corrcoef\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score, precision_score, recall_score, classification_report, log_loss, confusion_matrix, accuracy_score \n",
    "from sklearn.utils import class_weight\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import ml_insights as mli\n",
    "from betacal import BetaCalibration\n",
    "print(mli.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom functions to generate reliability diagram\n",
    "\n",
    "def calc_bins(y_test, preds):\n",
    "    num_bins = 10\n",
    "    bins = np.linspace(0.1, 1, num_bins)\n",
    "    binned = np.digitize(preds, bins)  \n",
    "    bin_accs = np.zeros(num_bins)\n",
    "    bin_confs = np.zeros(num_bins)\n",
    "    bin_sizes = np.zeros(num_bins)\n",
    "\n",
    "    for bin in range(num_bins):\n",
    "        bin_sizes[bin] = len(preds[binned == bin])\n",
    "    if bin_sizes[bin] > 0:\n",
    "        bin_accs[bin] = (y_test[binned==bin]).sum() / bin_sizes[bin]\n",
    "        bin_confs[bin] = (preds[binned==bin]).sum() / bin_sizes[bin]\n",
    "\n",
    "    return bins, binned, bin_accs, bin_confs, bin_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ECE and MCE metrics\n",
    "\n",
    "def get_metrics(y_test, preds):\n",
    "    ECE = 0\n",
    "    MCE = 0\n",
    "    bins, _, bin_accs, bin_confs, bin_sizes = calc_bins(y_test, preds)\n",
    "    for i in range(len(bins)):\n",
    "        abs_conf_dif = abs(bin_accs[i] - bin_confs[i])\n",
    "        ECE += (bin_sizes[i] / sum(bin_sizes)) * abs_conf_dif\n",
    "        MCE = max(MCE, abs_conf_dif)\n",
    "    return ECE,MCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw reliability diagram\n",
    "\n",
    "def draw_reliability_graph(y_test, preds):\n",
    "    ECE, MCE = get_metrics(y_test, preds)\n",
    "    bins, _, bin_accs, _, _ = calc_bins(y_test, preds)\n",
    "    fig = plt.figure(figsize=(15, 10), dpi=400)\n",
    "    ax = fig.gca()\n",
    "    # x/y limits\n",
    "    ax.set_xlim(0, 1.05)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=15)\n",
    "    # x/y labels\n",
    "    plt.xlabel('Prediction', fontsize=20)\n",
    "    plt.ylabel('Truth', fontsize=20)\n",
    "    # Create grid\n",
    "    ax.set_axisbelow(True) \n",
    "    ax.grid(color='gray', linestyle='dashed')\n",
    "    # Error bars\n",
    "    plt.bar(bins, bins,width=0.1,alpha=0.3,edgecolor='black', color='orange', hatch='\\\\')\n",
    "    # Draw bars and identity line\n",
    "    plt.bar(bins, bin_accs, width=0.1, alpha=1, \n",
    "          edgecolor='black', color='red') # b before \n",
    "    plt.plot([0,1],[0,1], '--', color='gray', linewidth=2)\n",
    "    # Equally spaced axes\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    # ECE and MCE legend\n",
    "    ECE_patch = mpatches.Patch(color='blue',  #green before\n",
    "                             label='ECE = {:.2f}%'.format(ECE*100))\n",
    "    MCE_patch = mpatches.Patch(color='green', \n",
    "                                label='MCE = {:.2f}%'.format(MCE*100))\n",
    "    plt.legend(handles=[ECE_patch, MCE_patch],prop={'size': 15})  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815383eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute performance metrics\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Kappa = 2 * (TP * TN - FN * FP) / (TP * FN + TP * FP + 2 * TP * TN + FN**2 + FN * TN + FP**2 + FP * TN)\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos','check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','F1','FBeta','MCC','BM','MK','Kappa'],     'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,F1,FBeta,MCC,BM,MK, Kappa]})\n",
    "    return (mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c77d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "'''\n",
    "Use the following to load data of all degrees of imbalances for the respective image modalities\n",
    "'''\n",
    "img_width, img_height = 256,256\n",
    "train_data_dir = \"data/train\"\n",
    "test_data_dir = \"data/test\"\n",
    "epochs = 32 \n",
    "batch_size = 32 \n",
    "num_classes = 2\n",
    "input_shape = (img_width, img_height, 3)\n",
    "model_input = Input(shape=input_shape)\n",
    "print(model_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd002ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define data generators\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.1) #90/10 train/val split\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        seed=42,\n",
    "        batch_size=batch_size, \n",
    "        class_mode='categorical', \n",
    "        subset = 'training')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        seed=42,\n",
    "        batch_size=batch_size, \n",
    "        class_mode='categorical', \n",
    "        subset = 'validation')\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size, \n",
    "        class_mode='categorical', \n",
    "        shuffle = False)\n",
    "\n",
    "#identify the number of samples\n",
    "nb_train_samples = len(train_generator.filenames)\n",
    "nb_validation_samples = len(validation_generator.filenames)\n",
    "nb_test_samples = len(test_generator.filenames)\n",
    "\n",
    "#check the class indices\n",
    "print(train_generator.class_indices)\n",
    "print(validation_generator.class_indices)\n",
    "print(test_generator.class_indices)\n",
    "\n",
    "#true labels\n",
    "Y_test=test_generator.classes\n",
    "print(Y_test.shape)\n",
    "\n",
    "#convert test labels to categorical\n",
    "Y_test1=to_categorical(Y_test, num_classes=num_classes, dtype='float32')\n",
    "print(Y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b221ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare model architecture\n",
    "\n",
    "vgg16_cnn = VGG16(include_top=False, weights='imagenet', \n",
    "                        input_tensor=model_input)\n",
    "base_model_vgg16=Model(inputs=vgg16_cnn.input,\n",
    "                        outputs=vgg16_cnn.get_layer('block5_conv3').output)\n",
    "x = base_model_vgg16.output    \n",
    "x = GlobalAveragePooling2D()(x)\n",
    "logits = Dense(num_classes, \n",
    "                    activation='softmax', name='predictions')(x)\n",
    "model_vgg16 = Model(inputs=base_model_vgg16.input, \n",
    "                    outputs=logits, \n",
    "                    name = 'vgg16_1')\n",
    "model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "\n",
    "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "model_vgg16.compile(optimizer=sgd, \n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=['accuracy']) \n",
    "filepath = 'model/' + model_vgg16.name + '.{epoch:02d}-{val_accuracy:.4f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_weights_only=False, \n",
    "                             save_best_only=True, \n",
    "                             mode='min', \n",
    "                             save_freq='epoch')\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                              patience=5, \n",
    "                              verbose=1, \n",
    "                              mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.5, \n",
    "                              patience=5,\n",
    "                              verbose=1,\n",
    "                              mode='min', \n",
    "                              min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, earlyStopping, reduce_lr]\n",
    "t=time.time()\n",
    "\n",
    "#reset generators\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "#train the model\n",
    "model_vgg16_history = model_vgg16.fit(train_generator, \n",
    "                                          steps_per_epoch=nb_train_samples // batch_size,\n",
    "                                  epochs=epochs, validation_data=validation_generator,\n",
    "                                  callbacks=callbacks_list, \n",
    "                                  validation_steps=nb_validation_samples // batch_size, \n",
    "                                  verbose=1)\n",
    "\n",
    "print('Training time: %s' % (time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08cc872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot performance\n",
    "\n",
    "N = 32 #change if early stopping\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(20,10), dpi=400)\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         model_vgg16_history.history[\"loss\"], 'orange', label=\"train_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         model_vgg16_history.history[\"val_loss\"], 'red', label=\"val_loss\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "          model_vgg16_history.history[\"accuracy\"], 'blue', label=\"train_acc\")\n",
    "plt.plot(np.arange(1, N+1), \n",
    "         model_vgg16_history.history[\"val_accuracy\"], 'green', label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"model_performance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and predict\n",
    "\n",
    "vgg16_model = load_model('model.h5')\n",
    "vgg16_model.summary()\n",
    "\n",
    "#compile the model\n",
    "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)  \n",
    "vgg16_model.compile(optimizer=sgd, \n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=['accuracy']) \n",
    "\n",
    "#Generate predictions on the test data\n",
    "test_generator.reset() \n",
    "custom_y_pred = vgg16_model.predict(test_generator,\n",
    "                                    nb_test_samples // batch_size, \n",
    "                                    verbose=1)\n",
    "custom_y_pred1_label = custom_y_pred.argmax(axis=-1)\n",
    "\n",
    "#save the predictions\n",
    "np.savetxt('y_pred.csv',\n",
    "           custom_y_pred,fmt='%f',delimiter = \",\")\n",
    "np.savetxt('_test.csv',\n",
    "           Y_test,fmt='%f',delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to save the predictions for each image along with the filenames to a CSV file\n",
    "\n",
    "predicted_class_indices=np.argmax(custom_y_pred,axis=1)\n",
    "print(predicted_class_indices)\n",
    "labels = (test_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "#save the results to a CSV file\n",
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":custom_y_pred1,\n",
    "                      \"Labels\":predictions})\n",
    "results.to_csv(\"results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd74d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need the scores of only the positive abnormal class\n",
    "\n",
    "custom_y_pred1 = custom_y_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for default predictions, the threshold is 0.5, vary this value for optimal PR-based thresholds\n",
    "\n",
    "custom_y_pred1_opt = np.where(custom_y_pred1 > 0.5, 1, 0) \n",
    "print(custom_y_pred1_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f108be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot reliability diagram\n",
    "\n",
    "preds_calibrated = custom_y_pred1\n",
    "draw_reliability_graph(Y_test, preds_calibrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cca3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also plot the calibration curves using the Scikit-learn package\n",
    "\n",
    "fig = plt.figure(1, figsize=(15, 10), dpi=400)\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax1.tick_params(axis='both', which='minor', labelsize=15)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax2.tick_params(axis='both', which='minor', labelsize=15)    \n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")    \n",
    "frac_of_pos, mean_pred_value = calibration_curve(Y_test, custom_y_pred1, n_bins=10)\n",
    "ax1.plot(mean_pred_value, frac_of_pos, \"s-\", label='Uncalibrated model', \n",
    "             color=\"red\", linewidth=4)\n",
    "ax1.set_ylabel(\"Truth\", fontsize=20)\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.legend(loc=\"upper right\", prop={\"size\":20})    \n",
    "ax2.hist(custom_y_pred1, range=(0, 1), bins=10, color = \"blue\", ec=\"red\",\n",
    "             label='Uncalibrated model', histtype=\"barstacked\", lw=2)     \n",
    "ax2.set_xlabel(\"Prediction\", fontsize=20)\n",
    "ax2.set_ylabel(\"Count\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return total number of bins and the number of samples in each bin\n",
    "\n",
    "def bin_total(y_true, y_prob, n_bins):\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "    return np.bincount(binids, minlength=len(bins))\n",
    "print(bin_total(Y_test,custom_y_pred1, n_bins=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print all metrics\n",
    "\n",
    "mat_met = matrix_metrix(Y_test1.argmax(axis=-1),\n",
    "                      custom_y_pred.argmax(axis=-1),\n",
    "                      beta=0.4)\n",
    "print (mat_met)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you vary the optimal threshold value then use this\n",
    "mat_met = matrix_metrix(Y_test1.argmax(axis=-1),\n",
    "                      custom_y_pred1_opt,\n",
    "                      beta=0.4)\n",
    "print (mat_met)\n",
    "\n",
    "# use these thresholded probabilites further to compute comfusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute Brier score loss\n",
    "\n",
    "print('The Brier Score Loss of the trained model is' , \n",
    "      round(brier_score_loss(Y_test,custom_y_pred[:,1]),4))\n",
    "\n",
    "#compute Log loss\n",
    "\n",
    "print('The Log Loss of the trained model is' , \n",
    "      round(log_loss(Y_test,custom_y_pred[:,1]),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836988be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the confusion matrix\n",
    "\n",
    "target_names = ['No-finding', 'TB'] #vary the labels for another imaging modality\n",
    "print(classification_report(Y_test1.argmax(axis=-1),\n",
    "                            custom_y_pred.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),\n",
    "                              custom_y_pred.argmax(axis=-1))\n",
    "np.set_printoptions(precision=5)\n",
    "x_axis_labels = ['No-finding', 'TB'] \n",
    "y_axis_labels = ['No-finding', 'TB'] \n",
    "plt.figure(figsize=(10,10), dpi=400)\n",
    "sns.set(font_scale=2)\n",
    "b = sns.heatmap(cnf_matrix, annot=True, square = True, \n",
    "            cbar=False, cmap='Greens', \n",
    "            annot_kws={'size': 30},\n",
    "            fmt='g', \n",
    "            xticklabels=x_axis_labels, \n",
    "            yticklabels=y_axis_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab45d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the ROC curves with optimum threshold\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, \n",
    "                                 custom_y_pred[:,1])\n",
    "auc_score=roc_auc_score(Y_test, custom_y_pred[:,1])\n",
    "print(auc_score)\n",
    "\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "\n",
    "# get the Youden's J statistic\n",
    "J = tpr - fpr\n",
    "\n",
    "# locate the index of the largest J statistic\n",
    "ix_youden = argmax(J)\n",
    "\n",
    "# locate the index of the largest g-means\n",
    "ix_gmeans = argmax(gmeans)\n",
    "\n",
    "#get the best threshold with G-means\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix_gmeans], gmeans[ix_gmeans]))\n",
    "\n",
    "#get the best threshold with Youden's J statistic\n",
    "print('Best Threshold=%f, Youden J statistic=%.3f' % (thresholds[ix_youden], J[ix_youden]))\n",
    "\n",
    "# Plot all ROC curves\n",
    "fig=plt.figure(figsize=(15,10), dpi=400)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_facecolor('white')\n",
    "major_ticks = np.arange(0.0, 1.1, 0.20) \n",
    "minor_ticks = np.arange(0.0, 1.1, 0.20)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, \n",
    "         label='No Skill')\n",
    "plt.plot(fpr, tpr, \n",
    "         marker='.',\n",
    "         markersize=12,\n",
    "         markerfacecolor='green',\n",
    "         linewidth=4,\n",
    "         color='red',\n",
    "         label='VGG-16')\n",
    "\n",
    "#select the optimum point using G-means\n",
    "plt.scatter(fpr[ix_gmeans], \n",
    "               tpr[ix_gmeans], \n",
    "               marker='X',           \n",
    "               s=300, color='blue', \n",
    "               label='Optimal threshold')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('False Positive Rate', fontsize=20)\n",
    "plt.ylabel('True Positive Rate', fontsize=20)\n",
    "plt.legend(loc=\"lower right\", prop={\"size\":20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal threshold on the PR curve that maximizes the F-score\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, \n",
    "                                 custom_y_pred[:,1])\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "#compute average precision\n",
    "average_precision_base = average_precision_score(Y_test, \n",
    "                                 custom_y_pred[:,1])\n",
    "print(\"The average precision value is\", average_precision_base)\n",
    "\n",
    "# area under the PR curve\n",
    "print(\"The area under the PR curve is\", metrics.auc(recall, precision))\n",
    "\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "\n",
    "# plot the PR curve for the model\n",
    "no_skill = len(Y_test[Y_test==1]) / len(Y_test)\n",
    "fig=plt.figure(figsize=(15,10), dpi=400)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_facecolor('white')\n",
    "major_ticks = np.arange(0.0, 1.1, 0.20) \n",
    "minor_ticks = np.arange(0.0, 1.1, 0.20)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "pyplot.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.plot(recall, precision, marker='.', color='red', label='VGG-16')\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='X', s=300, color='blue', label='Optimal threshold')\n",
    "# axis labels\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.legend(loc=\"lower right\", prop={\"size\":20})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8526415",
   "metadata": {},
   "source": [
    "### Model Calibration: \n",
    "##### We observed from the histogram that the probabilities produced by the model tend to assume extreme values. Therefore, calibration techniqueswere used to try and fix these distortions. Platt scaling is a logistic regression-based calibration method where we essentially just perform logistic regression on the output of the DL model (y_pred) with respect to the true class labels (Y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Platt scaling \n",
    "\n",
    "lr = LogisticRegression(C=99999999999, solver='lbfgs')\n",
    "# fit the model \n",
    "t=time.time()\n",
    "lr.fit(custom_y_pred1.reshape(-1,1), Y_test)\n",
    "print('Training time: %s' % (time.time()-t))\n",
    "custom_y_pred1_platts1 = lr.predict_proba(custom_y_pred1.reshape(-1,1))\n",
    "print('Prediction time: %s' % (time.time()-t))\n",
    "custom_y_pred1_platts = lr.predict_proba(custom_y_pred1.reshape(-1,1))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reliability diagram\n",
    "\n",
    "preds_calibrated = custom_y_pred1_platts\n",
    "draw_reliability_graph(Y_test,preds_calibrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print performance metrics\n",
    "\n",
    "mat_met = matrix_metrix(Y_test1.argmax(axis=-1),\n",
    "                      custom_y_pred1_platts1.argmax(axis=-1),\n",
    "                      beta=0.4)\n",
    "print (mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4d1e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you obtain the optimal threshold from the PR curve, use it\n",
    "\n",
    "custom_y_pred1_platts_opt= np.where(custom_y_pred1_platts > 0.6237, 1, 0) #this is just an example, use your optimal threshold\n",
    "print(custom_y_pred1_platts_opt)\n",
    "\n",
    "#compute performance metrics using this thresholded probability\n",
    "mat_met = matrix_metrix(Y_test1.argmax(axis=-1),\n",
    "                      custom_y_pred1_platts_opt,\n",
    "                      beta=0.4)\n",
    "print(mat_met)\n",
    "\n",
    "# use these thresholded probabilites further to compute comfusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafed12f",
   "metadata": {},
   "source": [
    "#### Use the codes for confusion matrix, ROC curves, and PR curves from before to plot these diagrams for the Platt-scaled probabilites. An example is given herewith. The same can be performed for all calibrated probabilites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "\n",
    "target_names = ['No-finding', 'TB'] \n",
    "print(classification_report(Y_test1.argmax(axis=-1),\n",
    "                            custom_y_pred1_platts1.argmax(axis=-1),\n",
    "                            target_names=target_names, digits=4))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_test1.argmax(axis=-1),\n",
    "                              custom_y_pred1_platts1.argmax(axis=-1))\n",
    "np.set_printoptions(precision=5)\n",
    "\n",
    "x_axis_labels = ['No-finding', 'TB']\n",
    "y_axis_labels = ['No-finding', 'TB']\n",
    "plt.figure(figsize=(10,10), dpi=400)\n",
    "sns.set(font_scale=2)\n",
    "b = sns.heatmap(cnf_matrix, annot=True, square = True, \n",
    "            cbar=False, cmap='Greens', \n",
    "            annot_kws={'size': 30},\n",
    "            fmt='g', \n",
    "            xticklabels=x_axis_labels, \n",
    "            yticklabels=y_axis_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f4b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute Brier score loss\n",
    "\n",
    "print('The Brier Score Loss of the trained model is' , \n",
    "      round(brier_score_loss(Y_test,custom_y_pred1_platts),4))\n",
    "\n",
    "#compute Log loss\n",
    "\n",
    "print('The Log Loss of the trained model is' , \n",
    "      round(log_loss(Y_test,custom_y_pred1_platts),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89669fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot roc curve and find the optimum threshold\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, \n",
    "                                 custom_y_pred1_platts)\n",
    "\n",
    "#compute area under the ROC curve\n",
    "auc_score=roc_auc_score(Y_test, custom_y_pred1_platts)\n",
    "print(auc_score)\n",
    "\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "\n",
    "# get the Youden's J statistic\n",
    "J = tpr - fpr\n",
    "\n",
    "# locate the index of the largest J statistic\n",
    "ix_youden = argmax(J)\n",
    "\n",
    "# locate the index of the largest g-means\n",
    "ix_gmeans = argmax(gmeans)\n",
    "\n",
    "#get the best threshold with G-means\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix_gmeans], gmeans[ix_gmeans]))\n",
    "\n",
    "#get the best threshold with Youden's J statistic\n",
    "print('Best Threshold=%f, Youden J statistic=%.3f' % (thresholds[ix_youden], J[ix_youden]))\n",
    "\n",
    "# Plot all ROC curves\n",
    "fig=plt.figure(figsize=(15,10), dpi=400)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_facecolor('white')\n",
    "major_ticks = np.arange(0.0, 1.1, 0.20) \n",
    "minor_ticks = np.arange(0.0, 1.1, 0.20)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, \n",
    "         label='No Skill')\n",
    "plt.plot(fpr, tpr, \n",
    "         marker='.',\n",
    "         markersize=12,\n",
    "         markerfacecolor='green',\n",
    "         linewidth=4,\n",
    "         color='red',\n",
    "         label='Platt-scaling')\n",
    "\n",
    "#select the optimum threshold using G-means\n",
    "plt.scatter(fpr[ix_gmeans], \n",
    "               tpr[ix_gmeans], \n",
    "               marker='X',           \n",
    "               s=300, color='blue', \n",
    "               label='Optimal threshold')\n",
    "\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('False Positive Rate', fontsize=20)\n",
    "plt.ylabel('True Positive Rate', fontsize=20)\n",
    "plt.legend(loc=\"lower right\", prop={\"size\":20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pr curves curves\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, \n",
    "                                 custom_y_pred1_platts)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "#compute average precision\n",
    "average_precision_platt = average_precision_score(Y_test, \n",
    "                                 custom_y_pred1_platts)\n",
    "print(\"The average precision value is\", average_precision_platt)\n",
    "\n",
    "# area under the PR curve\n",
    "print(\"The area under the PR curve is\", metrics.auc(recall, precision))\n",
    "\n",
    "# locate the index of the largest f score\n",
    "ix = argmax(fscore)\n",
    "\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore[ix]))\n",
    "# plot the PR curve for the model\n",
    "no_skill = len(Y_test[Y_test==1]) / len(Y_test)\n",
    "fig=plt.figure(figsize=(15,10), dpi=400)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_facecolor('white')\n",
    "major_ticks = np.arange(0.0, 1.1, 0.20) \n",
    "minor_ticks = np.arange(0.0, 1.1, 0.20)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "pyplot.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.plot(recall, precision, marker='.', color='red', label='Platt-scaling')\n",
    "pyplot.scatter(recall[ix], precision[ix], marker='X', s=300, color='blue', label='Optimal threshold')\n",
    "# axis labels\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.legend(loc=\"lower right\", prop={\"size\":20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a1b32",
   "metadata": {},
   "source": [
    "#### Beta calibration: Similar to Platt scaling with a couple of important improvements. It is a 3-parameter family of curves rather than 2-parameter Family of curves includes the line y=x (so it won't mess it up if it's already calibrated). \n",
    "\n",
    "##### Reference: Kull, M., Filho, T.S. & Flach, P.. (2017). Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers. Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, in PMLR 54:623-631"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit three-parameter beta calibration\n",
    "\n",
    "bc = BetaCalibration()\n",
    "t=time.time()\n",
    "bc.fit(custom_y_pred1, Y_test)\n",
    "print('Training time: %s' % (time.time()-t))\n",
    "#perform beta calibration\n",
    "t=time.time()\n",
    "custom_y_pred1_beta = bc.predict(custom_y_pred1)\n",
    "print('Prediction time: %s' % (time.time()-t))\n",
    "custom_y_pred1_beta_1 = np.array(1-custom_y_pred1_beta)\n",
    "custom_y_pred1_beta_2 = np.c_[custom_y_pred1_beta_1, custom_y_pred1_beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reliability diagram\n",
    "\n",
    "preds_calibrated = custom_y_pred1_beta\n",
    "draw_reliability_graph(Y_test, preds_calibrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b61d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metrics\n",
    "\n",
    "mat_met = matrix_metrix(Y_test1.argmax(axis=-1),\n",
    "                      custom_y_pred1_beta_2.argmax(axis=-1),\n",
    "                      beta=0.4)\n",
    "print (mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c8e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you obtain the optimal threshold from the PR curve, use it\n",
    "\n",
    "custom_y_pred1_beta_opt= np.where(custom_y_pred1_beta > 0.375305, 1, 0) #this is just an example, use your optimal threshold\n",
    "print(custom_y_pred1_beta_opt)\n",
    "\n",
    "#compute performance metrics using this thresholded probability\n",
    "mat_met = matrix_metrix(Y_test1.argmax(axis=-1),\n",
    "                      custom_y_pred1_beta_opt,\n",
    "                      beta=0.4)\n",
    "print(mat_met)\n",
    "\n",
    "# use these thresholded probabilites further to compute comfusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute Brier score loss\n",
    "\n",
    "print('The Brier Score Loss of the Beta calibrated model is' , \n",
    "      round(brier_score_loss(Y_test,custom_y_pred1_beta),4))\n",
    "\n",
    "#compute Log loss\n",
    "\n",
    "print('The Log Loss of the Beta calibrated model is' , \n",
    "      round(log_loss(Y_test,custom_y_pred1_beta),4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4067e0c7",
   "metadata": {},
   "source": [
    "#### SplineCalib fits a cubic smoothing spline to the relationship between the uncalibrated scores and the calibrated probabilities. Smoothing splines strike a balance between fitting the points well and having a smooth function. SplineCalib uses a smoothed logistic function - so the fit to data is measured by likelihood (i.e. log-loss) and the smoothness refers to the integrated second derivative before the logistic transformation. There is a nuisance parameter that trades off smoothness for fit. At one extreme it will revert to standard logistic regression (i.e. Platt scaling) and at the other extreme it will be a very wiggly function that fits the data but does not generalize well. SplineCalib automatically fits the nuisance parameter (though this can be adjusted by the user). The resulting calibration function is not necessarily monotonic. (In some cases this may be beneficial).\n",
    "\n",
    "##### Lucena, B. Spline-based Probability Calibration. https://arxiv.org/abs/1809.07751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust Spline paramters for a better fit suiting your problem\n",
    "\n",
    "splinecalib = mli.SplineCalib(penalty='l2',\n",
    "                              knot_sample_size=40,\n",
    "                              cv_spline=5,\n",
    "                              unity_prior=False,\n",
    "                              unity_prior_weight=128)\n",
    "\n",
    "t=time.time()\n",
    "splinecalib.fit(custom_y_pred1, Y_test)\n",
    "print('Training time: %s' % (time.time()-t))\n",
    "custom_y_pred1_spline = splinecalib.predict(custom_y_pred1)\n",
    "print('Prediction time: %s' % (time.time()-t))\n",
    "custom_y_pred1_spline_1 = np.array(1-custom_y_pred1_spline)\n",
    "custom_y_pred1_spline_2 = np.c_[custom_y_pred1_spline_1, custom_y_pred1_spline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01bc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reliability diagram\n",
    "\n",
    "preds_calibrated = custom_y_pred1_spline\n",
    "draw_reliability_graph(Y_test, preds_calibrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metrics\n",
    "\n",
    "mat_met = matrix_metrix(Y_test1.argmax(axis=-1),\n",
    "                      custom_y_pred1_spline_2.argmax(axis=-1),\n",
    "                      beta=0.4)\n",
    "print (mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1323bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you obtain the optimal threshold from the PR curve, use it\n",
    "\n",
    "custom_y_pred1_spline_opt= np.where(custom_y_pred1_spline > 0.375305, 1, 0) #this is just an example, use your optimal threshold\n",
    "print(custom_y_pred1_spline_opt)\n",
    "\n",
    "#compute performance metrics using this thresholded probability\n",
    "mat_met = matrix_metrix(Y_test1.argmax(axis=-1),\n",
    "                      custom_y_pred1_spline_opt,\n",
    "                      beta=0.4)\n",
    "print(mat_met)\n",
    "\n",
    "# use these thresholded probabilites further to compute comfusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute Brier score loss\n",
    "\n",
    "print('The Brier Score Loss of the Spline calibrated model is' , \n",
    "      round(brier_score_loss(Y_test,custom_y_pred1_spline),4))\n",
    "\n",
    "#compute Log loss\n",
    "\n",
    "print('The Log Loss of the Spline calibrated model is' , \n",
    "      round(log_loss(Y_test,custom_y_pred1_spline),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f539db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can plot the calibration curves together using the Scikit learn calibration curve method\n",
    "\n",
    "labels = []\n",
    "fig=plt.figure(figsize=(15,10), dpi=400)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_facecolor('white')\n",
    "major_ticks = np.arange(0.0, 1.1, 0.10) \n",
    "minor_ticks = np.arange(0.0, 1.1, 0.10)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=15)\n",
    "frac_of_positives_uncalibrated, pred_prob_uncalibrated = calibration_curve(Y_test,custom_y_pred1, n_bins=10)\n",
    "plt.plot(pred_prob_uncalibrated,\n",
    "         frac_of_positives_uncalibrated,\n",
    "         linewidth=4,\n",
    "         color='green')\n",
    "labels.append('Uncalibrated')\n",
    "frac_of_positives_platt, pred_prob_platt = calibration_curve(Y_test, custom_y_pred1_platts, n_bins=10)\n",
    "plt.plot(pred_prob_platt,\n",
    "         frac_of_positives_platt,\n",
    "         linewidth=4,\n",
    "         color='blue')\n",
    "labels.append('Platt scaling')\n",
    "frac_of_positives_beta, pred_prob_beta = calibration_curve(Y_test,custom_y_pred1_beta, n_bins=10)\n",
    "plt.plot(pred_prob_beta,\n",
    "          frac_of_positives_beta,\n",
    "          linewidth=4,\n",
    "          color='yellow')\n",
    "labels.append('Beta calibration')\n",
    "frac_of_positives_spline, pred_prob_spline = calibration_curve(Y_test,custom_y_pred1_spline, n_bins=10)\n",
    "plt.plot(pred_prob_spline,\n",
    "         frac_of_positives_spline,\n",
    "         linewidth=4,\n",
    "         color='red')\n",
    "labels.append('Spline calibration')\n",
    "plt.plot([0, 1], [0, 1], color='black', \n",
    "         linestyle='dashed', \n",
    "         linewidth = 1)\n",
    "labels.append('Perfectly calibrated')\n",
    "plt.legend(labels,loc=\"lower right\", prop={\"size\":20})\n",
    "xlabel = plt.xlabel(\"Prediction\", fontsize=20)\n",
    "ylabel = plt.ylabel(\"Truth\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can compare the PR curves of uncalibrated and calibrated curves and the optimal threshold values\n",
    "# an example is shown herewith\n",
    "\n",
    "precision1, recall1, thresholds1 = precision_recall_curve(Y_test, \n",
    "                                 custom_y_pred1_spline) # using spline-calibrated probabilities\n",
    "precision2, recall2, thresholds2 = precision_recall_curve(Y_test, \n",
    "                                 custom_y_pred[:,1]) # using baseline uncalibrated probabilities\n",
    "average_precision_spline = average_precision_score(Y_test, \n",
    "                                 custom_y_pred1_spline)\n",
    "print(\"The average precision value of Spline calibration is\", average_precision_spline)\n",
    "average_precision_base = average_precision_score(Y_test, \n",
    "                                 custom_y_pred[:,1])\n",
    "print(\"The average precision value of Baseline model is\", average_precision_base)\n",
    "fscore1 = (2 * precision1 * recall1) / (precision1 + recall1)\n",
    "fscore2 = (2 * precision2 * recall2) / (precision2 + recall2)\n",
    "\n",
    "# locate the index of the largest f score\n",
    "ix1 = argmax(fscore1)\n",
    "ix2 = argmax(fscore2)\n",
    "\n",
    "#best threshold with the calibration method\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds1[ix1], fscore1[ix1]))\n",
    "\n",
    "#best threshold with the baseline model\n",
    "print('Best Threshold=%f, F-Score=%.3f' % (thresholds2[ix2], fscore2[ix2]))\n",
    "\n",
    "# plot the PR curve for the model\n",
    "no_skill = len(Y_test[Y_test==1]) / len(Y_test)\n",
    "fig=plt.figure(figsize=(15,10), dpi=400)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_facecolor('white')\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "major_ticks = np.arange(0.0, 1.1, 0.20) \n",
    "minor_ticks = np.arange(0.0, 1.1, 0.20)\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.set_xticks(minor_ticks, minor=True)\n",
    "ax.set_yticks(major_ticks)\n",
    "ax.set_yticks(minor_ticks, minor=True)\n",
    "pyplot.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.plot(recall1, precision1, marker='.', color='black', label='Spline calibration') \n",
    "pyplot.plot(recall2, precision2, marker='.', color='red', label='Baseline')\n",
    "pyplot.scatter(recall1[ix1], precision1[ix1], marker='o', \n",
    "               s=300, color='blue', label='Optimal-Spline calibration')\n",
    "pyplot.scatter(recall2[ix2], precision2[ix2], marker='*', \n",
    "               s=400, color='yellow', label='Optimal-Baseline')\n",
    "# axis labels\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('Recall', fontsize=20)\n",
    "plt.ylabel('Precision', fontsize=20)\n",
    "plt.legend(loc=\"lower left\", prop={\"size\":20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce68cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7705e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b01fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169f2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc145e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c3edd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
